{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the end-to-end data processing pipeline for the Macro AI Agent.\n",
    "\n",
    "## Pipeline Stages\n",
    "\n",
    "1. **Ingestion**: Fetch raw data from sources\n",
    "2. **Cleaning**: Parse and structure the data\n",
    "3. **Enrichment**: Add classifications and metadata\n",
    "4. **Chunking**: Split into LLM-optimized chunks\n",
    "5. **Indexing**: Store in vector database\n",
    "\n",
    "**Updated**: 2025-11-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Ingestion (FRED Example)\n",
    "\n",
    "Fetch economic data from FRED API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRED API ingestion example\n",
    "from fredapi import Fred\n",
    "\n",
    "# Initialize FRED client\n",
    "FRED_API_KEY = os.getenv('FRED_API_KEY')\n",
    "fred = Fred(api_key=FRED_API_KEY)\n",
    "\n",
    "# Fetch CPI data\n",
    "print(\"Fetching CPI data from FRED...\")\n",
    "cpi_data = fred.get_series('CPIAUCSL', observation_start='2020-01-01')\n",
    "cpi_info = fred.get_series_info('CPIAUCSL')\n",
    "\n",
    "print(f\"\\nüìä Fetched {len(cpi_data)} observations\")\n",
    "print(f\"\\nSeries Info:\")\n",
    "print(f\"  Title: {cpi_info['title']}\")\n",
    "print(f\"  Units: {cpi_info['units']}\")\n",
    "print(f\"  Frequency: {cpi_info['frequency']}\")\n",
    "print(f\"  Last Updated: {cpi_info['last_updated']}\")\n",
    "\n",
    "# Preview data\n",
    "print(f\"\\nLatest 5 observations:\")\n",
    "print(cpi_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CPI data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cpi_data.index, cpi_data.values, linewidth=2)\n",
    "plt.title('Consumer Price Index (CPI) - All Urban Consumers', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Index (1982-1984=100)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate YoY inflation rate\n",
    "inflation_rate = cpi_data.pct_change(12) * 100  # 12-month change\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(inflation_rate.index, inflation_rate.values, linewidth=2, color='red')\n",
    "plt.axhline(y=2.0, color='green', linestyle='--', label='Fed Target (2%)')\n",
    "plt.title('Year-over-Year Inflation Rate', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Inflation Rate (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Current inflation rate: {inflation_rate.iloc[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Cleaning (FOMC Statement Example)\n",
    "\n",
    "Parse and clean a sample FOMC statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample FOMC statement (for demonstration)\n",
    "sample_fomc_statement = \"\"\"\n",
    "FOR RELEASE AT 2:00 P.M. EST\n",
    "March 20, 2024\n",
    "\n",
    "Recent indicators suggest that economic activity has been expanding at a solid pace. \n",
    "Job gains have been strong, and the unemployment rate has remained low. \n",
    "Inflation has eased over the past year but remains elevated. \n",
    "\n",
    "The Committee seeks to achieve maximum employment and inflation at the rate of 2 percent over the longer run. \n",
    "In support of these goals, the Committee decided to maintain the target range for the federal funds rate at 5-1/4 to 5-1/2 percent. \n",
    "\n",
    "The Committee will continue to assess additional information and its implications for monetary policy. \n",
    "In determining the extent of any additional policy firming that may be appropriate to return inflation to 2 percent over time, \n",
    "the Committee will take into account the cumulative tightening of monetary policy, the lags with which monetary policy affects \n",
    "economic activity and inflation, and economic and financial developments. \n",
    "\n",
    "In addition, the Committee will continue reducing its holdings of Treasury securities and agency debt and agency \n",
    "mortgage-backed securities. The Committee is strongly committed to returning inflation to its 2 percent objective.\n",
    "\"\"\"\n",
    "\n",
    "# Parse and clean\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_fomc_statement(text: str) -> dict:\n",
    "    \"\"\"Clean and structure FOMC statement.\"\"\"\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Extract date\n",
    "    date_match = re.search(r'(\\w+ \\d+, \\d{4})', text)\n",
    "    date_str = date_match.group(1) if date_match else None\n",
    "    \n",
    "    # Remove header\n",
    "    text = re.sub(r'FOR RELEASE AT.*?\\d{4}', '', text).strip()\n",
    "    \n",
    "    return {\n",
    "        \"title\": \"FOMC Statement\",\n",
    "        \"content\": text,\n",
    "        \"published_at\": date_str,\n",
    "        \"source_name\": \"Federal Reserve\",\n",
    "        \"content_type\": \"policy\",\n",
    "        \"macro_themes\": [\"monetary_policy\", \"inflation\"],\n",
    "        \"geography\": [\"us\"],\n",
    "        \"importance\": \"high\"\n",
    "    }\n",
    "\n",
    "cleaned_statement = clean_fomc_statement(sample_fomc_statement)\n",
    "\n",
    "print(\"‚úÖ Cleaned FOMC Statement:\")\n",
    "print(json.dumps(cleaned_statement, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Enrichment (Classification & Entity Extraction)\n",
    "\n",
    "Add AI-generated classifications and extract entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment classification (hawkish vs dovish)\n",
    "HAWKISH_PHRASES = [\n",
    "    \"persistent inflation\", \"inflation pressures\", \"further increases\",\n",
    "    \"additional tightening\", \"restrictive policy\", \"vigilant\",\n",
    "    \"committed to bringing inflation down\", \"policy firming\"\n",
    "]\n",
    "\n",
    "DOVISH_PHRASES = [\n",
    "    \"inflation has eased\", \"inflation has moderated\", \"disinflation\",\n",
    "    \"patient\", \"data-dependent\", \"judicious\", \"monitor\",\n",
    "    \"sufficient restrictiveness\", \"maintain\"\n",
    "]\n",
    "\n",
    "def classify_fomc_sentiment(text: str) -> dict:\n",
    "    \"\"\"Classify FOMC statement sentiment.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    hawkish_count = sum(1 for phrase in HAWKISH_PHRASES if phrase in text_lower)\n",
    "    dovish_count = sum(1 for phrase in DOVISH_PHRASES if phrase in text_lower)\n",
    "    \n",
    "    if hawkish_count > dovish_count + 1:\n",
    "        sentiment = \"hawkish\"\n",
    "    elif dovish_count > hawkish_count + 1:\n",
    "        sentiment = \"dovish\"\n",
    "    else:\n",
    "        sentiment = \"neutral\"\n",
    "    \n",
    "    return {\n",
    "        \"sentiment\": sentiment,\n",
    "        \"hawkish_signals\": hawkish_count,\n",
    "        \"dovish_signals\": dovish_count,\n",
    "        \"confidence\": abs(hawkish_count - dovish_count) / max(hawkish_count + dovish_count, 1)\n",
    "    }\n",
    "\n",
    "sentiment_analysis = classify_fomc_sentiment(cleaned_statement['content'])\n",
    "\n",
    "print(\"üìä Sentiment Analysis:\")\n",
    "print(json.dumps(sentiment_analysis, indent=2))\n",
    "\n",
    "# Add to document\n",
    "cleaned_statement['sentiment'] = sentiment_analysis['sentiment']\n",
    "cleaned_statement['ai_analysis'] = {\n",
    "    \"sentiment_details\": sentiment_analysis,\n",
    "    \"key_takeaways\": [\n",
    "        \"Fed maintains rates at 5.25-5.50%\",\n",
    "        \"Inflation has eased but remains elevated\",\n",
    "        \"Data-dependent approach to future policy\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity extraction (simple version - in production, use spaCy or LLM)\n",
    "import re\n",
    "\n",
    "def extract_entities(text: str) -> dict:\n",
    "    \"\"\"Extract entities from text.\"\"\"\n",
    "    \n",
    "    # Economic indicators (simple regex matching)\n",
    "    indicators = []\n",
    "    indicator_patterns = [\n",
    "        r\"\\b(inflation|CPI|unemployment|GDP|jobs?|employment)\\b\",\n",
    "        r\"\\b(federal funds rate|interest rate|rates?)\\b\",\n",
    "        r\"\\b(Treasury|securities|mortgage-backed)\\b\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in indicator_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        indicators.extend(set(match.lower() for match in matches))\n",
    "    \n",
    "    # Organizations\n",
    "    organizations = []\n",
    "    if re.search(r\"\\b(Committee|FOMC|Federal Reserve|Fed)\\b\", text, re.IGNORECASE):\n",
    "        organizations.append(\"Federal Reserve\")\n",
    "        organizations.append(\"FOMC\")\n",
    "    \n",
    "    # Policy actions\n",
    "    actions = []\n",
    "    if re.search(r\"maintain.*target range\", text, re.IGNORECASE):\n",
    "        actions.append(\"maintain_rates\")\n",
    "    if re.search(r\"reduc.*holdings\", text, re.IGNORECASE):\n",
    "        actions.append(\"quantitative_tightening\")\n",
    "    \n",
    "    return {\n",
    "        \"indicators\": list(set(indicators)),\n",
    "        \"organizations\": list(set(organizations)),\n",
    "        \"policy_actions\": actions\n",
    "    }\n",
    "\n",
    "entities = extract_entities(cleaned_statement['content'])\n",
    "cleaned_statement['entities'] = entities\n",
    "\n",
    "print(\"üè¢ Extracted Entities:\")\n",
    "print(json.dumps(entities, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Chunking (LLM Optimization)\n",
    "\n",
    "Split document into chunks for vector embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking strategy\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Count tokens in text.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def chunk_document(doc: dict, chunk_size: int = 800, chunk_overlap: int = 100) -> list:\n",
    "    \"\"\"Chunk document for LLM retrieval.\"\"\"\n",
    "    \n",
    "    # Prepend metadata for context\n",
    "    metadata_prefix = f\"\"\"Source: {doc['source_name']} | Date: {doc['published_at']} | Type: {doc['content_type']}\n",
    "Themes: {', '.join(doc['macro_themes'])}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Initialize splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        length_function=count_tokens\n",
    "    )\n",
    "    \n",
    "    # Split content\n",
    "    content_chunks = splitter.split_text(doc['content'])\n",
    "    \n",
    "    # Create chunk objects\n",
    "    chunks = []\n",
    "    for i, chunk_text in enumerate(content_chunks):\n",
    "        # Prepend metadata\n",
    "        full_chunk = metadata_prefix + chunk_text\n",
    "        \n",
    "        chunks.append({\n",
    "            \"chunk_id\": f\"{doc.get('id', 'doc')}_chunk_{i}\",\n",
    "            \"chunk_index\": i,\n",
    "            \"text\": full_chunk,\n",
    "            \"token_count\": count_tokens(full_chunk),\n",
    "            # Inherit metadata from parent\n",
    "            \"published_at\": doc['published_at'],\n",
    "            \"source_name\": doc['source_name'],\n",
    "            \"content_type\": doc['content_type'],\n",
    "            \"macro_themes\": doc['macro_themes'],\n",
    "            \"geography\": doc['geography'],\n",
    "            \"importance\": doc['importance']\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Chunk the FOMC statement\n",
    "chunks = chunk_document(cleaned_statement)\n",
    "\n",
    "print(f\"‚úÇÔ∏è Created {len(chunks)} chunks\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Tokens: {chunk['token_count']}\")\n",
    "    print(f\"  Preview: {chunk['text'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Embedding & Indexing (Vector Database)\n",
    "\n",
    "Generate embeddings and prepare for vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings (using OpenAI ada-002)\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def generate_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list:\n",
    "    \"\"\"Generate embedding for text.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(\"Generating embeddings...\")\n",
    "for chunk in chunks:\n",
    "    chunk['embedding'] = generate_embedding(chunk['text'])\n",
    "    print(f\"  ‚úÖ Chunk {chunk['chunk_index']}: {len(chunk['embedding'])} dimensions\")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated embeddings for {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Document JSON (Final Output)\n",
    "\n",
    "This is what gets stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble final document\n",
    "import uuid\n",
    "\n",
    "final_document = {\n",
    "    \"id\": str(uuid.uuid4()),\n",
    "    \"url\": \"https://www.federalreserve.gov/newsevents/pressreleases/monetary20240320a.htm\",\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \n",
    "    # Content\n",
    "    \"title\": cleaned_statement['title'],\n",
    "    \"content\": cleaned_statement['content'],\n",
    "    \"summary\": \"Fed maintains rates at 5.25-5.50%, noting inflation has eased but remains elevated. Data-dependent approach continues.\",\n",
    "    \"language\": \"en\",\n",
    "    \n",
    "    # Source\n",
    "    \"source_name\": cleaned_statement['source_name'],\n",
    "    \"source_type\": \"central_bank\",\n",
    "    \"source_credibility\": 1.0,\n",
    "    \"is_paywalled\": False,\n",
    "    \n",
    "    # Temporal\n",
    "    \"published_at\": \"2024-03-20T14:00:00Z\",\n",
    "    \"ingested_at\": datetime.now().isoformat(),\n",
    "    \"effective_date\": \"2024-03-20\",\n",
    "    \n",
    "    # Classification\n",
    "    \"content_type\": cleaned_statement['content_type'],\n",
    "    \"macro_themes\": cleaned_statement['macro_themes'],\n",
    "    \"geography\": cleaned_statement['geography'],\n",
    "    \"asset_classes\": [\"equities\", \"fixed_income\", \"fx\"],\n",
    "    \"sentiment\": cleaned_statement['sentiment'],\n",
    "    \"importance\": cleaned_statement['importance'],\n",
    "    \n",
    "    # Entities\n",
    "    \"entities\": cleaned_statement['entities'],\n",
    "    \n",
    "    # AI Analysis\n",
    "    \"ai_analysis\": cleaned_statement['ai_analysis'],\n",
    "    \n",
    "    # Chunks (without embeddings for display)\n",
    "    \"chunks\": [\n",
    "        {k: v for k, v in chunk.items() if k != 'embedding'}\n",
    "        for chunk in chunks\n",
    "    ],\n",
    "    \n",
    "    # Provenance\n",
    "    \"ingestion_method\": \"web_scraping\",\n",
    "    \"processing_version\": \"v1.0\",\n",
    "    \"verification_status\": \"verified\",\n",
    "    \"human_reviewed\": False\n",
    "}\n",
    "\n",
    "print(\"üì¶ Final Document Structure:\")\n",
    "print(json.dumps(final_document, indent=2)[:2000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage (Supabase Insert)\n",
    "\n",
    "Insert into Supabase database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supabase insertion (pseudocode - requires supabase-py)\n",
    "\"\"\"\n",
    "from supabase import create_client\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase = create_client(\n",
    "    os.getenv('SUPABASE_URL'),\n",
    "    os.getenv('SUPABASE_KEY')\n",
    ")\n",
    "\n",
    "# Insert document\n",
    "response = supabase.table('documents').insert(final_document).execute()\n",
    "\n",
    "# Insert chunks\n",
    "for chunk in chunks:\n",
    "    chunk_data = {\n",
    "        'document_id': final_document['id'],\n",
    "        'chunk_index': chunk['chunk_index'],\n",
    "        'text': chunk['text'],\n",
    "        'embedding': chunk['embedding'],\n",
    "        'token_count': chunk['token_count'],\n",
    "        # ... metadata\n",
    "    }\n",
    "    supabase.table('chunks').insert(chunk_data).execute()\n",
    "\n",
    "print(\"‚úÖ Document and chunks inserted into database\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚ö†Ô∏è Supabase insertion code (requires database setup)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Test (Semantic Search)\n",
    "\n",
    "Test vector similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate vector search\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "# User query\n",
    "user_query = \"What is the Fed's stance on interest rates?\"\n",
    "\n",
    "# Generate query embedding\n",
    "query_embedding = generate_embedding(user_query)\n",
    "\n",
    "# Calculate similarity with each chunk\n",
    "similarities = []\n",
    "for chunk in chunks:\n",
    "    similarity = cosine_similarity(query_embedding, chunk['embedding'])\n",
    "    similarities.append({\n",
    "        'chunk_index': chunk['chunk_index'],\n",
    "        'similarity': similarity,\n",
    "        'text': chunk['text'][:200]  # Preview\n",
    "    })\n",
    "\n",
    "# Sort by similarity\n",
    "similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "print(f\"üîç Query: '{user_query}'\\n\")\n",
    "print(\"Top 2 most relevant chunks:\\n\")\n",
    "for i, result in enumerate(similarities[:2]):\n",
    "    print(f\"Rank {i+1}:\")\n",
    "    print(f\"  Similarity: {result['similarity']:.4f}\")\n",
    "    print(f\"  Text: {result['text']}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "1. ‚úÖ **Ingestion**: Fetched FRED data and parsed FOMC statement\n",
    "2. ‚úÖ **Cleaning**: Structured raw data into unified format\n",
    "3. ‚úÖ **Enrichment**: Added sentiment, entities, AI analysis\n",
    "4. ‚úÖ **Chunking**: Split into LLM-optimized chunks with metadata\n",
    "5. ‚úÖ **Embedding**: Generated vector embeddings\n",
    "6. ‚úÖ **Retrieval**: Tested semantic search\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Implement full ingestion modules in `src/ingestion/`\n",
    "2. Set up Supabase database with schema from `DATA_SCHEMA.md`\n",
    "3. Build processing pipeline in `src/ingestion/processors.py`\n",
    "4. Implement vector store in `src/knowledge/vector_store.py`\n",
    "5. Test with more data sources (RSS, research PDFs)\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Metadata is crucial**: Prepending context improves retrieval\n",
    "- **Chunking strategy matters**: Balance between context and specificity\n",
    "- **Enrichment adds value**: Sentiment and entities help filtering\n",
    "- **Human-readable + LLM-optimized**: Store both formats\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to implement the full pipeline!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
